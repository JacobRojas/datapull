{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"DataPull DataPull is a self-service tool provided by HomeAway's Data Tools team to migrate data across heterogeneous datastores effortlessly. When deployed to Amazon AWS, DataPull spins up EMR Spark infrastructure, does the data movement and terminates the infrastructure once the job is complete; to minimize costs. Multiple data migrations can be done ither serially or in parallel within a DataPull job. There also exists built-in integration with Hashicorp Vault so that datastore credentials are never exposed. DataPull also has a built-in scheduler for daily/recurring jobs; or its REST API endpoints can be invoked by a third-party scheduler. DataPull supports the following datastores as sources and destinations. Platform Source Destination SQL Server \u2714 \u2714 Cassandra \u2714 \u2714 Mongodb \u2714 \u2714 S3 \u2714 \u2714 FileSystem \u2714 \u2714 SFTP \u2714 \u2714 Elasticsearch \u2714 \u2714 Kafka \u2714 X Neo4j X \u2714 MySql \u2714 \u2714 Postgres \u2714 \u2714 InfluxDB \u2714 \u2714 How to use DataPull Steps common to all environments (Dev/Test/Stage/Prod)... Create a json file/string that has the source/s, destination/s and the portfolio information for tagging the ephemeral infrastructure needed to do the DataPull Here are some sample JSON files for some common use cases: https://github.com/homeaway/datapull/blob/master/core/src/main/resources/Samples Here is JSON specification document which has all the possible options supported by DataPull: https://github.com/homeaway/datapull/blob/master/core/src/main/resources/Samples/Input_Json_Specification.json Please provide an email address for the element \"useremailaddress\" in the JSON input. Once your DataPull completes, an email will be sent with all the migration details along with the source, destination details of the migrations, the time taken for the migrations to complete and the number of records processed, etc. For DataPull to get access to the source and destination data platforms, you need to either provide the login and password within the JSON (not recommended for Stage and Production environments); or provide the login and an IAM Role which is mapped to the credentials in Vault (you also need to add the \"awsenv\" and \"vaultenv\" elements to the source(s) and destination in the json input). Navigate to your swagger api URL and invoke this REST API with the json as input. More information about the Input JSON: The input JSON mainly divided into Three sections. Basic Information about the Job Migrations Cluster The basic information section is there for allowing the users to put their email address, enabling the jobs to run in parallel or serial etc. Migrations section is an array and primarily covers source and the destination details of a migration. It can have multiple migration objects. And enabling the any options/features is very easy, we just have to add the element as specified in the Input_specifications document which is available in the resources folder. If you want to migrate only set of columns of the source data, DataPull supports this by using Mappings array inside a migration object and Please check the sample JSONs available for a example. Cluster section is mostly related to the infrastructure of the migration job which has details about the portfolio, product, Cron expression and environment in which the job will be running etc. The cluster will be spun up based on the awsenv(aws environment) element in this section among dev/test/stage/prod. This sample JSON input moves data from MySql to AWS S3 What we use to make the DataPull do it's job? Spark is the main engine which drives the DataPull. So technically it will run on any spark environment. But we are biased to Amazon's Elastic Map Reduce(EMR) to have minimum dependencies(AWS versus AWS+Qubole) and our app does use EMR to spin up the spark clusters. But after all as it is a spark application it will run on any spark cluster whether it is EMR or Qubole or any other spark clusters. DataPull expects the Input JSON as an argument and irrespective of any spark environment we have to pass the Input JSON as an argument to make the DataPull work. Run your DataPull on a spark cluster If you are starting from scratch... Prepare the Input JSON as explained in the above section with all the required information. Then please go to swagger API URL and submit the JSON in the inputJson field provided and click Try it out! For Elastic Search In the field of mappingid it is recommended to use _id or id as fieldnames as they are a unique keyword in Elastic Search. How to use delta DataPulls? So lets say you did a DataPull, however in the second datapull you do not want to repopulate everything but only need the deltas of new records. Here is what is needed. Pre-req: CDC needs to enabled on the database Setup CDC for your specified tables. It creates a watermarking table which essentially allows you to calculate the deltas. Next is you create your json but with Pre-migrate and Post migrate steps. In your pre-migrate step you populate your watermarking table with defaults and you update the watermark_from with watermark to field. And as part of post migration field you reset the watermark_to field with now date so you are ready for the future datapull Once this is complete you can revert back to the original of how to use DataPull again. How to schedule a DataPull ? Please add an element cronexpression in the cluster section of the Input Json. For example, \"cronexpression\": \"0 21 * * *\" executes DataPull every day at 9 PM UTC.","title":"Home"},{"location":"#datapull","text":"DataPull is a self-service tool provided by HomeAway's Data Tools team to migrate data across heterogeneous datastores effortlessly. When deployed to Amazon AWS, DataPull spins up EMR Spark infrastructure, does the data movement and terminates the infrastructure once the job is complete; to minimize costs. Multiple data migrations can be done ither serially or in parallel within a DataPull job. There also exists built-in integration with Hashicorp Vault so that datastore credentials are never exposed. DataPull also has a built-in scheduler for daily/recurring jobs; or its REST API endpoints can be invoked by a third-party scheduler. DataPull supports the following datastores as sources and destinations. Platform Source Destination SQL Server \u2714 \u2714 Cassandra \u2714 \u2714 Mongodb \u2714 \u2714 S3 \u2714 \u2714 FileSystem \u2714 \u2714 SFTP \u2714 \u2714 Elasticsearch \u2714 \u2714 Kafka \u2714 X Neo4j X \u2714 MySql \u2714 \u2714 Postgres \u2714 \u2714 InfluxDB \u2714 \u2714","title":"DataPull"},{"location":"#how-to-use-datapull","text":"","title":"How to use DataPull"},{"location":"#steps-common-to-all-environments-devteststageprod","text":"Create a json file/string that has the source/s, destination/s and the portfolio information for tagging the ephemeral infrastructure needed to do the DataPull Here are some sample JSON files for some common use cases: https://github.com/homeaway/datapull/blob/master/core/src/main/resources/Samples Here is JSON specification document which has all the possible options supported by DataPull: https://github.com/homeaway/datapull/blob/master/core/src/main/resources/Samples/Input_Json_Specification.json Please provide an email address for the element \"useremailaddress\" in the JSON input. Once your DataPull completes, an email will be sent with all the migration details along with the source, destination details of the migrations, the time taken for the migrations to complete and the number of records processed, etc. For DataPull to get access to the source and destination data platforms, you need to either provide the login and password within the JSON (not recommended for Stage and Production environments); or provide the login and an IAM Role which is mapped to the credentials in Vault (you also need to add the \"awsenv\" and \"vaultenv\" elements to the source(s) and destination in the json input). Navigate to your swagger api URL and invoke this REST API with the json as input.","title":"Steps common to all environments (Dev/Test/Stage/Prod)..."},{"location":"#more-information-about-the-input-json","text":"The input JSON mainly divided into Three sections. Basic Information about the Job Migrations Cluster The basic information section is there for allowing the users to put their email address, enabling the jobs to run in parallel or serial etc. Migrations section is an array and primarily covers source and the destination details of a migration. It can have multiple migration objects. And enabling the any options/features is very easy, we just have to add the element as specified in the Input_specifications document which is available in the resources folder. If you want to migrate only set of columns of the source data, DataPull supports this by using Mappings array inside a migration object and Please check the sample JSONs available for a example. Cluster section is mostly related to the infrastructure of the migration job which has details about the portfolio, product, Cron expression and environment in which the job will be running etc. The cluster will be spun up based on the awsenv(aws environment) element in this section among dev/test/stage/prod. This sample JSON input moves data from MySql to AWS S3","title":"More information about the Input JSON:"},{"location":"#what-we-use-to-make-the-datapull-do-its-job","text":"Spark is the main engine which drives the DataPull. So technically it will run on any spark environment. But we are biased to Amazon's Elastic Map Reduce(EMR) to have minimum dependencies(AWS versus AWS+Qubole) and our app does use EMR to spin up the spark clusters. But after all as it is a spark application it will run on any spark cluster whether it is EMR or Qubole or any other spark clusters. DataPull expects the Input JSON as an argument and irrespective of any spark environment we have to pass the Input JSON as an argument to make the DataPull work.","title":"What we use to make the DataPull do it's job?"},{"location":"#run-your-datapull-on-a-spark-cluster","text":"","title":"Run your DataPull on a spark cluster"},{"location":"#if-you-are-starting-from-scratch","text":"Prepare the Input JSON as explained in the above section with all the required information. Then please go to swagger API URL and submit the JSON in the inputJson field provided and click Try it out!","title":"If you are starting from scratch..."},{"location":"#for-elastic-search-in-the-field-of-mappingid-it-is-recommended-to-use-_id-or-id-as-fieldnames-as-they-are-a-unique-keyword-in-elastic-search","text":"","title":"For Elastic Search In the field of mappingid it is recommended to use _id or id as fieldnames as they are a unique keyword in Elastic Search."},{"location":"#how-to-use-delta-datapulls","text":"So lets say you did a DataPull, however in the second datapull you do not want to repopulate everything but only need the deltas of new records. Here is what is needed. Pre-req: CDC needs to enabled on the database Setup CDC for your specified tables. It creates a watermarking table which essentially allows you to calculate the deltas. Next is you create your json but with Pre-migrate and Post migrate steps. In your pre-migrate step you populate your watermarking table with defaults and you update the watermark_from with watermark to field. And as part of post migration field you reset the watermark_to field with now date so you are ready for the future datapull Once this is complete you can revert back to the original of how to use DataPull again.","title":"How to use delta DataPulls?"},{"location":"#how-to-schedule-a-datapull","text":"Please add an element cronexpression in the cluster section of the Input Json. For example, \"cronexpression\": \"0 21 * * *\" executes DataPull every day at 9 PM UTC.","title":"How to schedule a DataPull ?"},{"location":"architecture/","text":"","title":"Architecture"},{"location":"emr_runbook/","text":"Runbook for using DataPull deployed on AWS EMR 1. Prepare the Pipeline JSON 1.1 Here's a sample JSON that joins Property information from SQL Server with units in cassandra and moves to s3. Please copy this to your favorite text editor { \"useremailaddress\": \"<EMAIL_ADDRESS>\", \"migrations\": [ { \"sources\": [ { \"platform\": \"mssql\", \"server\": \"server_name\", \"database\": \"db_name\", \"table\": \"properties_table\", \"login\": \"user_id\", \"password\": \"password\", \"alias\": \"properties\" }, { \"platform\": \"cassandra\", \"cluster\": \"cassandra_server_name\", \"keyspace\": \"keyspace_name\", \"table\": \"units\", \"login\": \"user_id\", \"password\": \"password\", \"alias\": \"units\" } ], \"destination\": { \"platform\": \"s3\", \"s3path\": \"bucket_name/path/\", \"fileformat\": \"csv\" }, \"sql\": { \"query\": \"select properties.pro_id, properties.pro_city, units.unituuid from properties JOIN units ON properties.pro_id = units.propertyid limit 100\" } } ], \"cluster\": { \"pipelinename\": \"emr_cluster_name\", \"awsenv\": \"dev\", \"portfolio\": \"portfolio_name\", \"product\": \"product_name\", \"ec2instanceprofile\": \"ec2_instance_profile\", \"terminateclusterafterexecution\": \"false\", \"ComponentInfo\":\"YOUR_Component-UUID_dominion\" } } 1.2 In the JSON in your text editor, please replace <EMAIL_ADDRESS> with your email address. database and cluster details with your database and cluster details. 2. Submit the JSON to the API 2.1 Copy the modified JSON from your text editor, and paste it into the swagger API inputJson. Swagger API can be accessed through URL - http://IP-Address:8080/swagger-ui.html#!/data45pull45request45handler/startDatapull . If api is deployed in ECS FARGATE, replace the IP-Address with IP address of task of FARGATE app. If it is deployed in local, replace IP-Address with localhost. 2.2 Click the \"Try it out!\" button to queue the data pull. 3. Check status of the job We can check the status of the job in spark UI. Spark UI runs on the master node of the EMR cluster. URL of the Spark UI will be http://master-node-IP:8088/cluster So, to find the master node IP address ,login into your AWS console, navigate to EMR service and find your cluster. If pipelinename is datapull and awsenv is dev, the cluster name will be dev-emr-datapull-pipeline. Find the IP address of the master node from hardware tab. 4. Get the email confirmation Wait for a few minutes, and check your email to get a confirmation when your data pull job is complete. 5. Check S3 for your data Log into your AWS console. Navigate to your S3 bucket. Check the data under s3 path provided in the input json.","title":"Running DataPull on AWS EMR"},{"location":"emr_runbook/#runbook-for-using-datapull-deployed-on-aws-emr","text":"","title":"Runbook for using DataPull deployed on AWS EMR"},{"location":"emr_runbook/#1-prepare-the-pipeline-json","text":"1.1 Here's a sample JSON that joins Property information from SQL Server with units in cassandra and moves to s3. Please copy this to your favorite text editor { \"useremailaddress\": \"<EMAIL_ADDRESS>\", \"migrations\": [ { \"sources\": [ { \"platform\": \"mssql\", \"server\": \"server_name\", \"database\": \"db_name\", \"table\": \"properties_table\", \"login\": \"user_id\", \"password\": \"password\", \"alias\": \"properties\" }, { \"platform\": \"cassandra\", \"cluster\": \"cassandra_server_name\", \"keyspace\": \"keyspace_name\", \"table\": \"units\", \"login\": \"user_id\", \"password\": \"password\", \"alias\": \"units\" } ], \"destination\": { \"platform\": \"s3\", \"s3path\": \"bucket_name/path/\", \"fileformat\": \"csv\" }, \"sql\": { \"query\": \"select properties.pro_id, properties.pro_city, units.unituuid from properties JOIN units ON properties.pro_id = units.propertyid limit 100\" } } ], \"cluster\": { \"pipelinename\": \"emr_cluster_name\", \"awsenv\": \"dev\", \"portfolio\": \"portfolio_name\", \"product\": \"product_name\", \"ec2instanceprofile\": \"ec2_instance_profile\", \"terminateclusterafterexecution\": \"false\", \"ComponentInfo\":\"YOUR_Component-UUID_dominion\" } } 1.2 In the JSON in your text editor, please replace <EMAIL_ADDRESS> with your email address. database and cluster details with your database and cluster details.","title":"1. Prepare the Pipeline JSON"},{"location":"emr_runbook/#2-submit-the-json-to-the-api","text":"2.1 Copy the modified JSON from your text editor, and paste it into the swagger API inputJson. Swagger API can be accessed through URL - http://IP-Address:8080/swagger-ui.html#!/data45pull45request45handler/startDatapull . If api is deployed in ECS FARGATE, replace the IP-Address with IP address of task of FARGATE app. If it is deployed in local, replace IP-Address with localhost. 2.2 Click the \"Try it out!\" button to queue the data pull.","title":"2. Submit the JSON to the API"},{"location":"emr_runbook/#3-check-status-of-the-job","text":"We can check the status of the job in spark UI. Spark UI runs on the master node of the EMR cluster. URL of the Spark UI will be http://master-node-IP:8088/cluster So, to find the master node IP address ,login into your AWS console, navigate to EMR service and find your cluster. If pipelinename is datapull and awsenv is dev, the cluster name will be dev-emr-datapull-pipeline. Find the IP address of the master node from hardware tab.","title":"3. Check status of the job"},{"location":"emr_runbook/#4-get-the-email-confirmation","text":"Wait for a few minutes, and check your email to get a confirmation when your data pull job is complete.","title":"4. Get the email confirmation"},{"location":"emr_runbook/#5-check-s3-for-your-data","text":"Log into your AWS console. Navigate to your S3 bucket. Check the data under s3 path provided in the input json.","title":"5. Check S3 for your data"},{"location":"faq/","text":"FAQs Can I use datapull to bulk delete rows in my Cassandra database? The SQL used for migrating Cassandra to Cassandra cannot be used to issue DELETE s, but it can update the TTL on rows using a spark option. To run a bulk delete, just set both the source and destination to the same table, select only the rows that you want to delete, and set a small TTL on them using a nonzero value in the spark.cassandra.output.ttl field in your sparkoptions block. (This is technically only an update, but it will have the desired effect of deleting the selected rows when the TTL runs out.) Example: In a table with columns foo , bar , and baz , the following block in your input JSON could be used to delete all rows where baz is set to true . { \"migrations\": [ { \"source\": { ... \"table\": \"my_foo_table\", \"alias\": \"source_table\" }, \"sql\":{ \"query\": \"SELECT foo, bar, baz FROM source_table WHERE baz = true\" }, \"destination\": { ... \"table\": \"my_foo_table\", \"sparkoptions\" : { \"spark.cassandra.output.ttl\" : \"1\" } } } ] } !!! note For Elastic Search as a platform, it is recommended not to use _id or id as field names of mappingid as they are unique keyword in Elastic Search. How can I submit a DataPull job which should run exactly once? To make the DataPull job to run only once, we can remove the cronexpression parameter from the json, then it will be executed only once. How to replace or upsert documents in MongoDB using DataPull? We can use replace documents by setting up the option replacedocuments to true (By default it is true) which will replace the whole document with the recent one which is having the same _id. If we want to upsert new columns to the existing documents we can the replacedocuments to false and we have to explicitly select use the same _id along with the new columns and in case if we don't have the same _id on the source side then we can read destination mongo collection as one of the source and joining that with the other source/s then pushing it to the destination will upsert the documents existing. How does DataPull behave when reading data from MongoDB collection which is having more than one schema? DataPull uses dataframes to move data from source/s to destination. Dataframes are schema bound i.e it expects a single schema across the whole dataset. To find the schema of a Mongo Collection, DataPull will sample a few documents in the Collection (the sample size is configurable). To conclude Datapull can't reliably migrate a collection/dataset which has multiple schemas in it. How do I convert an Array of Doubles/Strings to a string using Spark SQL? For example - \"categories\":[\"geoAdmin:continent\",\"meta:highLevelRegion\"] then we can use CONCAT('[',concat_ws(',',categories),']') as categories . How do I convert Array of Jsons to a String? For example - \"localizedNames\":[{\"lcid\":1025,\"value\":\"\u0623\u0646\u062a\u0627\u0631\u0643\u062a\u064a\u0643\u0627\",\"extendedValue\":\"\u0623\u0646\u062a\u0627\u0631\u0643\u062a\u064a\u0643\u0627\"},{\"lcid\":1028,\"value\":\"\u5357\u6975\u6d32\",\"extendedValue\":\"\u5357\u6975\u6d32\"}] then we can use to_json(localizedNames) as localizedNames How do I convert String UUID to Type4 UUID? For moving data to MongoDB with custom _id or any UUID's as UUID than as a String, we wrote a custom function. we can use uuidToBinary(uuid_colum) as _id in the sql column of the json. and can use binaryToUUID(_id) as column_name to do the vice versa. For any Spark SQL functions please refer to https://spark.apache.org/docs/2.2.0/api/java/org/apache/spark/sql/functions.html and If you don't find any, feel free to let us know we are happy to write the custom function or you can contribute back to the tool. How to I create a complex document structure with nested arrays and subdocuments, using Spark SQL? Let's assume you want to create an array within a document within an array within a document, like this ... { \"unitUuid\": \"00000000-0000-0000-0000-000000000000\", \"unitInfo\": [ { \"UnitID\": \"3000523\", \"Y\": [ \"204785\" ] } ] } Here's the Spark SQL statement that will produce the above document. select UA.unitUuid , UA.cibEnabled , ( select collect_set ( named_struct( 'UnitID', U.unitId , 'Y', ( select collect_set(P.listingNumber) from P where U.propertyEntityId = P._id ) ) ) from U where UA.unitEntityId = U._id ) as unitInfo from UA Common errors and their fixes EMR Pipeline errors Symptom You submit a DataPull job either through the UI or through the REST API endpoint and the EMR cluster isn't created. Fixes Check if IAM role or secret and access key given to API service is having permission to create EMR cluster. If this is a scheduled job (i.e you have specified a cron expression in your JSON input) the pipeline will not start building until the scheduled time (which is in UTC) becomes current. If this is an ad-hoc job (i.e. you haven't specified a cron expression in your JSON input) the system automatically schedules the pipeline to run approximately 5 minutes from the time you submitted the JSON through the UI/REST api endpoint. Symptom You are unable to move data from an S3 bucket in Dev environment to an InfluxDB cluster in Production environment Fixes This is a known limitation that affects InfluxDB alone; and there is an easy workaround - Do a DataPull from the S3 bucket in the Dev environment to an S3 bucket in the Production environment; using a Spark cluster in the Production environment. - You will need to provide the AWS access key and secret key for the Dev environment, in the input json. - You need not provide the AWS access key and secret key for the Production environment, in the input json since the IAM role that the Spark cluster runs on would/should usually have access to the S3 buckets in the same environment. - Do a DataPull from the S3 bucket in the Production environment to the InfluxDB cluster in the production environment; using a Spark cluster in the Production environment. !!! note \"Please note that\" - this limitation does not affect any other source-destination pair as of 2019-01-08 i.e. you can move data from a dev S3 bucket to a production MongoDB cluster; you can move data from a dev Cassandra cluster to a production InfluxDb cluster etc. - if Production Isolation is fully enforced i.e. once there is no network access between the production and non-production environments, no tool including DataPull will be able to move data between production and non-Production environments without approval from the Security team and an exemption from the Network team. Vault errors Symptom You get the following error in the email report java.io.IOException: Server returned HTTP response code: 403 for URL: https://vault-url/clustername/login at Fixes The issue here is that Vault was unable to find the password for your clustername and loginname. Please check the following The clustername and login name are case-sensitive. The clustername in Vault does not match the name you provided in Vault. Getting error when reading from a cassandra table from the local environment? Error: INFO FileFormatWriter: Job null committed. Exception in thread \u201cmain\u201d java.lang.NoClassDefFoundError: org/apache/commons/configuration/ConfigurationException at org.apache.spark.sql.cassandra.DefaultSource$.<init>(DefaultSource.scala:135) at org.apache.spark.sql.cassandra.DefaultSource$.<clinit>(DefaultSource.scala) Fixes Please add the below dependency to the core pom and re run the job. <dependency> <groupId>commons-configuration</groupId> <artifactId>commons-configuration</artifactId> <version>1.10</version> </dependency>","title":"FAQs"},{"location":"faq/#faqs","text":"","title":"FAQs"},{"location":"faq/#can-i-use-datapull-to-bulk-delete-rows-in-my-cassandra-database","text":"The SQL used for migrating Cassandra to Cassandra cannot be used to issue DELETE s, but it can update the TTL on rows using a spark option. To run a bulk delete, just set both the source and destination to the same table, select only the rows that you want to delete, and set a small TTL on them using a nonzero value in the spark.cassandra.output.ttl field in your sparkoptions block. (This is technically only an update, but it will have the desired effect of deleting the selected rows when the TTL runs out.) Example: In a table with columns foo , bar , and baz , the following block in your input JSON could be used to delete all rows where baz is set to true . { \"migrations\": [ { \"source\": { ... \"table\": \"my_foo_table\", \"alias\": \"source_table\" }, \"sql\":{ \"query\": \"SELECT foo, bar, baz FROM source_table WHERE baz = true\" }, \"destination\": { ... \"table\": \"my_foo_table\", \"sparkoptions\" : { \"spark.cassandra.output.ttl\" : \"1\" } } } ] } !!! note For Elastic Search as a platform, it is recommended not to use _id or id as field names of mappingid as they are unique keyword in Elastic Search.","title":"Can I use datapull to bulk delete rows in my Cassandra database?"},{"location":"faq/#how-can-i-submit-a-datapull-job-which-should-run-exactly-once","text":"To make the DataPull job to run only once, we can remove the cronexpression parameter from the json, then it will be executed only once.","title":"How can I submit a DataPull job which should run exactly once?"},{"location":"faq/#how-to-replace-or-upsert-documents-in-mongodb-using-datapull","text":"We can use replace documents by setting up the option replacedocuments to true (By default it is true) which will replace the whole document with the recent one which is having the same _id. If we want to upsert new columns to the existing documents we can the replacedocuments to false and we have to explicitly select use the same _id along with the new columns and in case if we don't have the same _id on the source side then we can read destination mongo collection as one of the source and joining that with the other source/s then pushing it to the destination will upsert the documents existing.","title":"How to replace or upsert documents in MongoDB using DataPull?"},{"location":"faq/#how-does-datapull-behave-when-reading-data-from-mongodb-collection-which-is-having-more-than-one-schema","text":"DataPull uses dataframes to move data from source/s to destination. Dataframes are schema bound i.e it expects a single schema across the whole dataset. To find the schema of a Mongo Collection, DataPull will sample a few documents in the Collection (the sample size is configurable). To conclude Datapull can't reliably migrate a collection/dataset which has multiple schemas in it.","title":"How does DataPull behave when reading data from MongoDB collection which is having more than one schema?"},{"location":"faq/#how-do-i-convert-an-array-of-doublesstrings-to-a-string-using-spark-sql","text":"For example - \"categories\":[\"geoAdmin:continent\",\"meta:highLevelRegion\"] then we can use CONCAT('[',concat_ws(',',categories),']') as categories .","title":"How do I convert an Array of Doubles/Strings to a string  using Spark SQL?"},{"location":"faq/#how-do-i-convert-array-of-jsons-to-a-string","text":"For example - \"localizedNames\":[{\"lcid\":1025,\"value\":\"\u0623\u0646\u062a\u0627\u0631\u0643\u062a\u064a\u0643\u0627\",\"extendedValue\":\"\u0623\u0646\u062a\u0627\u0631\u0643\u062a\u064a\u0643\u0627\"},{\"lcid\":1028,\"value\":\"\u5357\u6975\u6d32\",\"extendedValue\":\"\u5357\u6975\u6d32\"}] then we can use to_json(localizedNames) as localizedNames","title":"How do I convert Array of Jsons to a String?"},{"location":"faq/#how-do-i-convert-string-uuid-to-type4-uuid","text":"For moving data to MongoDB with custom _id or any UUID's as UUID than as a String, we wrote a custom function. we can use uuidToBinary(uuid_colum) as _id in the sql column of the json. and can use binaryToUUID(_id) as column_name to do the vice versa. For any Spark SQL functions please refer to https://spark.apache.org/docs/2.2.0/api/java/org/apache/spark/sql/functions.html and If you don't find any, feel free to let us know we are happy to write the custom function or you can contribute back to the tool.","title":"How do I convert String UUID to Type4 UUID?"},{"location":"faq/#how-to-i-create-a-complex-document-structure-with-nested-arrays-and-subdocuments-using-spark-sql","text":"Let's assume you want to create an array within a document within an array within a document, like this ... { \"unitUuid\": \"00000000-0000-0000-0000-000000000000\", \"unitInfo\": [ { \"UnitID\": \"3000523\", \"Y\": [ \"204785\" ] } ] } Here's the Spark SQL statement that will produce the above document. select UA.unitUuid , UA.cibEnabled , ( select collect_set ( named_struct( 'UnitID', U.unitId , 'Y', ( select collect_set(P.listingNumber) from P where U.propertyEntityId = P._id ) ) ) from U where UA.unitEntityId = U._id ) as unitInfo from UA","title":"How to I create a complex document structure with nested arrays and subdocuments, using Spark SQL?"},{"location":"faq/#common-errors-and-their-fixes","text":"","title":"Common errors and their fixes"},{"location":"faq/#emr-pipeline-errors","text":"","title":"EMR Pipeline errors"},{"location":"faq/#symptom","text":"You submit a DataPull job either through the UI or through the REST API endpoint and the EMR cluster isn't created.","title":"Symptom"},{"location":"faq/#fixes","text":"Check if IAM role or secret and access key given to API service is having permission to create EMR cluster. If this is a scheduled job (i.e you have specified a cron expression in your JSON input) the pipeline will not start building until the scheduled time (which is in UTC) becomes current. If this is an ad-hoc job (i.e. you haven't specified a cron expression in your JSON input) the system automatically schedules the pipeline to run approximately 5 minutes from the time you submitted the JSON through the UI/REST api endpoint.","title":"Fixes"},{"location":"faq/#symptom_1","text":"You are unable to move data from an S3 bucket in Dev environment to an InfluxDB cluster in Production environment","title":"Symptom"},{"location":"faq/#fixes_1","text":"This is a known limitation that affects InfluxDB alone; and there is an easy workaround - Do a DataPull from the S3 bucket in the Dev environment to an S3 bucket in the Production environment; using a Spark cluster in the Production environment. - You will need to provide the AWS access key and secret key for the Dev environment, in the input json. - You need not provide the AWS access key and secret key for the Production environment, in the input json since the IAM role that the Spark cluster runs on would/should usually have access to the S3 buckets in the same environment. - Do a DataPull from the S3 bucket in the Production environment to the InfluxDB cluster in the production environment; using a Spark cluster in the Production environment. !!! note \"Please note that\" - this limitation does not affect any other source-destination pair as of 2019-01-08 i.e. you can move data from a dev S3 bucket to a production MongoDB cluster; you can move data from a dev Cassandra cluster to a production InfluxDb cluster etc. - if Production Isolation is fully enforced i.e. once there is no network access between the production and non-production environments, no tool including DataPull will be able to move data between production and non-Production environments without approval from the Security team and an exemption from the Network team.","title":"Fixes"},{"location":"faq/#vault-errors","text":"","title":"Vault errors"},{"location":"faq/#symptom_2","text":"You get the following error in the email report java.io.IOException: Server returned HTTP response code: 403 for URL: https://vault-url/clustername/login at","title":"Symptom"},{"location":"faq/#fixes_2","text":"The issue here is that Vault was unable to find the password for your clustername and loginname. Please check the following The clustername and login name are case-sensitive. The clustername in Vault does not match the name you provided in Vault.","title":"Fixes"},{"location":"faq/#getting-error-when-reading-from-a-cassandra-table-from-the-local-environment","text":"Error: INFO FileFormatWriter: Job null committed. Exception in thread \u201cmain\u201d java.lang.NoClassDefFoundError: org/apache/commons/configuration/ConfigurationException at org.apache.spark.sql.cassandra.DefaultSource$.<init>(DefaultSource.scala:135) at org.apache.spark.sql.cassandra.DefaultSource$.<clinit>(DefaultSource.scala)","title":"Getting error when reading from a cassandra table from the local environment?"},{"location":"faq/#fixes_3","text":"Please add the below dependency to the core pom and re run the job. <dependency> <groupId>commons-configuration</groupId> <artifactId>commons-configuration</artifactId> <version>1.10</version> </dependency>","title":"Fixes"},{"location":"install_on_aws/","text":"Deploying DataPull on AWS ECS Fargate and EMR The following steps describe how to deploy the DataPull app in a fresh AWS account. In a nutshell, these steps do the following - Creates an IAM user that has enough permissions to create a ECS Fargate app, and upload DataPull JARs to S3 - Creates an IAM role that has enough permissions to create EMR clusters, and read/write to S3 DataPull App Structure DataPull application is divided into two components i.e. api and core . Core contains the actual spark logic that moves data across databases. It is the component that actually does the data transfer. Api contains code that allows accessing DataPull services. It exposes the REST endpoint that accepts the input json. The API internally schedules the job, saves DataPull configuration into S3 and manages EMR lifecycle for spark job. Pre-requisites Clone/download the master branch of this repo Have an AWS account Have available, the AWS access key <aws_admin_access_key> and secret key <aws_admin_secret_key> of an IAM user that can create IAM users and IAM roles in your AWS account It is advisable that AWS access key <aws_admin_access_key> and secret key <aws_admin_secret_key> have admin access to the AWS account. Hence, this step would be typically done by the team managing the AWS account. Have a S3 bucket <datapull_s3_bucket> (this bucket can be an existing bucket or a new bucket) in which the folder datapull-opensource will be used to store DataPull's artifacts and logs Know a valid AWS VPC subnet id <aws_subnetid_private> in the region <aws_region> accessible to your clients, ideally not accessible from the internet; but has access to the internet Know a valid AWS security group <aws_security_grp> in the region <aws_region> accessible to your clients, ideally not accessible from the internet; but has access to the internet Have Docker installed on the machine used for deployment Configure DataPull master_application_config.yml(present in project root directory) configuration parameters datapull.application.region(Optional) : This specifies the regions supported by AWS. If you specify this value, you don't have to separately specify region for s3 buckets and emr cluster. This parameter is mandatory if you don't specify emr region and s3 region in api config. datapull.logger.s3.loggingfolder : This parameter is used to store spark logs in s3. Give a valid S3 bucket path to store logs. For example {bucket_name}/datapull/logs/ is a valid s3 path. As of now this is a mandatory. datapull.logger.cloudwatch(Optional) : Required only if you want DataPull core to put log events into cloudwatch. DataPull api uses api/src/main/resources/logback-spring.xml for configuring logging. For DataPull api component to put logs into cloudwatch, please enable cloudwatch logging in api/src/main/resources/logback-spring.xml . datapull.logger.cloudwatch.accessKey and datapull.logger.cloudwatch.secretKey needs to be specified only if the machine running the application(core and api part) doesn't have the desired IAM roles. datapull.logger.mssql(Optional) : Required only you want to log events into mssql. In case you want to use this feature please specify all the paramters required within this node. ddatapull.email.smtp(Optional) : This is required if you want to specify task completion mail from DataPull. If you want to use SMTP server to send you emails, values of all the properties within this node has to be specified. You can also configure AWS SES to send emails. datapull.email.ses(Optional) : This property configures aws ses to send DataPull task completion email. email property within this node is the AWS SES verified email. datapull.email.ses.accessKey and datapull.email.ses.secretKey needs to be specified only if the machine running the application(core and api part) doesn't have the desired IAM roles. Note: from the above datapull.email.smtp and datapull.email.ses properties, one of the properties is required to be configured. If both of these properties are not configured, there is no way for DataPull to send process completion mail. DataPull API configuration parameters (api/src/main/resources/application-${env}.yml) datapull.api.s3_bucket_name : Specify DataPull S3 bucket name here. This is the bucket where DataPull input jsons and logs are stored. datapull.api.s3_bucket_region(Optional) : DataPull S3 bucket region. Needs to be specified only if datapull.application.region is not specified in master_application_config.yml or S3 region is different from datapull.application.region specified in master_application_config.yml. datapull.api.s3_jar_path : This is the path of DataPull core jar in S3. You can keep it in any S3 bucket provided your app has access to this bucket. By default you can keep s3 path as s3:// /datapull-opensource/jars/DataMigrationFramework-1.0-SNAPSHOT-jar-with-dependencies.jar datapull.api.application_subnet : subnet for your application. The fargate app will be deployed within this subnet. If emr_subnet is not provided for EMR cluster, the emr cluster will also be created within this subnet. datapull.api.application_security_group : security group for fargate app. This security group can also be used for emr clusters, but as of now security group for emr are created by EMR sdk API. Note: from the above properties, datapull.api.s3_bucket_region needs to be specified only if datapull.application.region is not specified in master_application_config.yml or S3 region is different from datapull.application.region specified in master_application_config.yml . datapull.emr.emr_security_group_master(Optional) : Security group for EMR master node. datapull.emr.emr_security_group_slave(Optional) : Security group for EMR slave node. datapull.emr.emr_security_group_service_access(Optional) : Service access security group for EMR. This security group allows accessing spark UI through web browser. datapull.emr.emr_region(Optional) : DataPull EMR region. Needs to be specified only if datapull.application.region is not specified in master_application_config.yml or EMR region is different from datapull.application.region specified in master_application_config.yml . datapull.emr.tags(Optional) : Tag for emr cluster. Note: from the above properties properties, datapull.emr.emr_region needs to be specified only if datapull.application.region is not specified in master_application_config.yml or emr region is different from datapull.application.region specified in master_application_config.yml . In the file api/src/main/resources/terraform/datapull_task/datapull_ecs.tf set the HCL resource attribute aws_ecs_service.datapullwebapi_service.network_configuration.assign_public_ip to false if the VPC subnet id <aws_subnetid_private> is accessible from your clients' network; else set this to true Note: Setting this value to has obvious security risks since this exposes the DataPull endpoint to the internet. Use this for demo purposes only. DataPull Core configuration paramters(core/src/main/resources/application.yml) datapull.secretstore.vault(Optional) : Required only if application is using vault to access credentials. Configure the properties within this yml node to make DataPull work with vault. Oracle and Teradata support For performing Data migration on Oracle and Teradata, the respective jar needs to be manually downloaded from their Company's website and add dependency in pom.xml file. Steps to download ojdbc jar Go to the url https://www.oracle.com/technetwork/apps-tech/jdbc-112010-090769.html Accept the license agreement by clicking of the radio button in front of the 'Accept License Agreement'. Oracle would ask you to create an account if you don't have one already. If you already have an account you can simply login to your account. Click on ojdbc6.jar. Download will start in few seconds. Steps to download teradata jar Go to the url https://downloads.teradata.com/download/connectivity/jdbc-driver. For downloading teradata jar, account needs to be created on teradata website. Downloads will be available in either .tar or .zip format. Archive file name will be in the format TeraJDBC__indep_indep_{version}.zip. Download the archive file. The archive file with contain two jar files terajdbc4.jar and tdgssconfig.jar. Both jars are needed for teradata jdbc functionality to work. Steps to include Oracle into the project Run this command to include Oracle jar into maven repo. Run this command from folder where Oracle jars are present. Change the value of -Dversion=11.2.0.3 in the command according to downloaded jar version. Command : docker run -e MAVEN_OPTS=\"-Xmx1024M -Xss128M -XX:MetaspaceSize=512M -XX:MaxMetaspaceSize=1024M -XX:+CMSClassUnloadingEnabled\" --rm -v $(pwd):/workdir -v $HOME/.m2/:/root/.m2/ -w /workdir maven:alpine mvn install:install-file -Dfile=ojdbc6.jar -DgroupId=com.oracle -DartifactId=ojdbc6 -Dversion=11.2.0.3 -Dpackaging=jar Add below mentioned dependency to pom.xml(core/pom.xml) in DataPull core. Replace {version} with the downloaded jar version. <dependency> <groupId>com.oracle</groupId> <artifactId>ojdbc6</artifactId> <version>{version}</version> </dependency> Steps to include Teradata into the project Run these commands to include Teradata jars into maven repo. Run this command from folder where Teradata jars are present. Change the value of -Dversion=16.20.00.10 in the command according to downloaded jar version. Commands : docker run -e MAVEN_OPTS=\"-Xmx1024M -Xss128M -XX:MetaspaceSize=512M -XX:MaxMetaspaceSize=1024M -XX:+CMSClassUnloadingEnabled\" --rm -v $(pwd):/workdir -v $HOME/.m2/:/root/.m2/ -w /workdir maven:alpine mvn install:install-file -Dfile=terajdbc4.jar -DgroupId=com.teradata -DartifactId=terajdbc4 -Dversion=16.20.00.10 -Dpackaging=jar docker run -e MAVEN_OPTS=\"-Xmx1024M -Xss128M -XX:MetaspaceSize=512M -XX:MaxMetaspaceSize=1024M -XX:+CMSClassUnloadingEnabled\" --rm -v $(pwd):/workdir -v $HOME/.m2/:/root/.m2/ -w /workdir maven:alpine mvn install:install-file -Dfile=tdgssconfig.jar -DgroupId=com.teradata -DartifactId=tdgssconfig -Dversion=16.20.00.10 -Dpackaging=jar Add below mentioned dependency to pom.xml(core/pom.xml) in DataPull core. Replace {version} with the downloaded jar version. <dependency> <groupId>com.teradata</groupId> <artifactId>terajdbc4</artifactId> <version>{version}</version> </dependency> <dependency> <groupId>com.teradata</groupId> <artifactId>tdgssconfig</artifactId> <version>{version}</version> </dependency> Create DataPull Infrastructure Have available, the AWS access key <aws_admin_access_key> and secret key <aws_admin_secret_key> of an IAM user that can create IAM users and IAM roles in your AWS account It is advisable that AWS access key <aws_admin_access_key> and secret key <aws_admin_secret_key> have admin access to the AWS account. Hence, this step would be typically done by the team managing the AWS account. Have a S3 bucket <datapull_s3_bucket> (this bucket can be an existing bucket or a new bucket) in which the folder datapull-opensource will be used to store DataPull's artifacts and logs Know a valid AWS VPC subnet id <aws_subnetid_private> in the region <aws_region> accessible to your clients, ideally not accessible from the internet; but has access to the internet Know a valid AWS security group <aws_security_grp> in the region <aws_region> accessible to your clients, ideally not accessible from the internet; but has access to the internet Have Docker installed on the machine used for deployment The Oracle ojdbc jar should be present in the resources folder of DataPull core component i.e. core/src/main/resources/ojdbc-jar/. The process of downloading ojdbc jar is explained at the end of this file. Create IAM User and Roles, with policies From the terminal at the root of the repo, run cd api/src/main/resources/terraform/datapull_iam/ chmod +x create_user_and_roles.sh Run ./create_user_and_roles.sh <aws_admin_access_key> <aws_admin_secret_key> <aws_region> <datapull_s3_bucket> <docker_image_name> . The script will produce a ton of stdout data, that ends with Outputs: datapull_user_access_key = <datapull_user_access_key> datapull_user_secret_key = <datapull_user_secret_key> Record <datapull_user_access_key> and <datapull_user_secret_key> for use in the next section Create AWS Fargate API App From the terminal (assuming you are using the same terminal session as the previous steps), run cd ./../datapull_task/ If you're at the root of the repo, run cd api/src/main/resources/terraform/datapull_task/ chmod +x ecs_deploy.sh ./ecs_deploy.sh <datapull_user_access_key> <datapull_user_secret_key> <docker_image_name> <env> Browse to the DataPull API swagger endpoint On AWS Console, navigate to Services > Elastic Container Service > Clusters > datapullwebapi > Tasks > <id_of_task> If the HCL resource attribute aws_ecs_service.datapullwebapi_service.network_configuration.assign_public_ip was set to false, take the Private IP of the task as <datapull_api_ip> else take this as the Public IP Open the API endpoint url http://<datapull_api_ip>:8080/swagger-ui.html#!/data45pull45request45handler/startDataPullUsingPOST Do your first DataPull Create a csv file at the S3 location s3://<datapull_s3_bucket>/datapull-opensource/data/firstdatapull/source/helloworld.csv with the following data hellofield,worldfield hello,world Post the following JSON input to the API endpoint url http://<datapull_api_ip>:8080/swagger-ui.html#!/data45pull45request45handler/startDataPullUsingPOST . Please remember to replace <your_id@DOMAIN.com> and <datapull_s3_bucket> with valid data. { \"useremailaddress\": \"<your_id@DOMAIN.com>\", \"precisecounts\": true, \"migrations\": [ { \"sources\": [ { \"platform\": \"s3\", \"s3path\": \"<datapull_s3_bucket>/datapull-opensource/data/firstdatapull/source\", \"fileformat\": \"csv\", \"savemode\": \"Overwrite\" } ], \"destination\": { \"platform\": \"s3\", \"s3path\": \"<datapull_s3_bucket>/datapull-opensource/data/firstdatapull/destination\", \"fileformat\": \"json\", \"savemode\": \"Overwrite\" } } ], \"cluster\": { \"pipelinename\": \"firstdatapull\", \"awsenv\": \"dev\", \"portfolio\": \"Data Engineering Services\", \"terminateclusterafterexecution\": \"true\", \"product\": \"Data Engineering Data Tools\", \"ComponentInfo\": \"00000000-0000-0000-0000-000000000000\" } } In approximately 8 minutes (to account for the time taken for the ephemeral EMR cluster to spin up), you should get the data from the CSV converted into JSON and written to the S3 folder s3://<datapull_s3_bucket>/datapull-opensource/data/firstdatapull/destination/ The logs for each DataPull invocation are available at s3://<datapull_s3_bucket>/datapull-opensource/logs/DataPullHistory . The logs for each migration with DataPull invocations are available at s3://<datapull_s3_bucket>/datapull-opensource/logs/MigrationHistory If you had configured the anonymous SMTP server for DataPull to send you email reports, you could get a report with the subject DataPull Report - firstdatapull (application_<random_numbers>)","title":"Installing DataPull on AWS Fargate and S3"},{"location":"install_on_aws/#deploying-datapull-on-aws-ecs-fargate-and-emr","text":"The following steps describe how to deploy the DataPull app in a fresh AWS account. In a nutshell, these steps do the following - Creates an IAM user that has enough permissions to create a ECS Fargate app, and upload DataPull JARs to S3 - Creates an IAM role that has enough permissions to create EMR clusters, and read/write to S3","title":"Deploying DataPull on AWS ECS Fargate and EMR"},{"location":"install_on_aws/#datapull-app-structure","text":"DataPull application is divided into two components i.e. api and core . Core contains the actual spark logic that moves data across databases. It is the component that actually does the data transfer. Api contains code that allows accessing DataPull services. It exposes the REST endpoint that accepts the input json. The API internally schedules the job, saves DataPull configuration into S3 and manages EMR lifecycle for spark job.","title":"DataPull App Structure"},{"location":"install_on_aws/#pre-requisites","text":"Clone/download the master branch of this repo Have an AWS account Have available, the AWS access key <aws_admin_access_key> and secret key <aws_admin_secret_key> of an IAM user that can create IAM users and IAM roles in your AWS account It is advisable that AWS access key <aws_admin_access_key> and secret key <aws_admin_secret_key> have admin access to the AWS account. Hence, this step would be typically done by the team managing the AWS account. Have a S3 bucket <datapull_s3_bucket> (this bucket can be an existing bucket or a new bucket) in which the folder datapull-opensource will be used to store DataPull's artifacts and logs Know a valid AWS VPC subnet id <aws_subnetid_private> in the region <aws_region> accessible to your clients, ideally not accessible from the internet; but has access to the internet Know a valid AWS security group <aws_security_grp> in the region <aws_region> accessible to your clients, ideally not accessible from the internet; but has access to the internet Have Docker installed on the machine used for deployment","title":"Pre-requisites"},{"location":"install_on_aws/#configure-datapull","text":"","title":"Configure DataPull"},{"location":"install_on_aws/#master_application_configymlpresent-in-project-root-directory-configuration-parameters","text":"datapull.application.region(Optional) : This specifies the regions supported by AWS. If you specify this value, you don't have to separately specify region for s3 buckets and emr cluster. This parameter is mandatory if you don't specify emr region and s3 region in api config. datapull.logger.s3.loggingfolder : This parameter is used to store spark logs in s3. Give a valid S3 bucket path to store logs. For example {bucket_name}/datapull/logs/ is a valid s3 path. As of now this is a mandatory. datapull.logger.cloudwatch(Optional) : Required only if you want DataPull core to put log events into cloudwatch. DataPull api uses api/src/main/resources/logback-spring.xml for configuring logging. For DataPull api component to put logs into cloudwatch, please enable cloudwatch logging in api/src/main/resources/logback-spring.xml . datapull.logger.cloudwatch.accessKey and datapull.logger.cloudwatch.secretKey needs to be specified only if the machine running the application(core and api part) doesn't have the desired IAM roles. datapull.logger.mssql(Optional) : Required only you want to log events into mssql. In case you want to use this feature please specify all the paramters required within this node. ddatapull.email.smtp(Optional) : This is required if you want to specify task completion mail from DataPull. If you want to use SMTP server to send you emails, values of all the properties within this node has to be specified. You can also configure AWS SES to send emails. datapull.email.ses(Optional) : This property configures aws ses to send DataPull task completion email. email property within this node is the AWS SES verified email. datapull.email.ses.accessKey and datapull.email.ses.secretKey needs to be specified only if the machine running the application(core and api part) doesn't have the desired IAM roles. Note: from the above datapull.email.smtp and datapull.email.ses properties, one of the properties is required to be configured. If both of these properties are not configured, there is no way for DataPull to send process completion mail.","title":"master_application_config.yml(present in project root directory) configuration parameters"},{"location":"install_on_aws/#datapull-api-configuration-parameters-apisrcmainresourcesapplication-envyml","text":"datapull.api.s3_bucket_name : Specify DataPull S3 bucket name here. This is the bucket where DataPull input jsons and logs are stored. datapull.api.s3_bucket_region(Optional) : DataPull S3 bucket region. Needs to be specified only if datapull.application.region is not specified in master_application_config.yml or S3 region is different from datapull.application.region specified in master_application_config.yml. datapull.api.s3_jar_path : This is the path of DataPull core jar in S3. You can keep it in any S3 bucket provided your app has access to this bucket. By default you can keep s3 path as s3:// /datapull-opensource/jars/DataMigrationFramework-1.0-SNAPSHOT-jar-with-dependencies.jar datapull.api.application_subnet : subnet for your application. The fargate app will be deployed within this subnet. If emr_subnet is not provided for EMR cluster, the emr cluster will also be created within this subnet. datapull.api.application_security_group : security group for fargate app. This security group can also be used for emr clusters, but as of now security group for emr are created by EMR sdk API. Note: from the above properties, datapull.api.s3_bucket_region needs to be specified only if datapull.application.region is not specified in master_application_config.yml or S3 region is different from datapull.application.region specified in master_application_config.yml . datapull.emr.emr_security_group_master(Optional) : Security group for EMR master node. datapull.emr.emr_security_group_slave(Optional) : Security group for EMR slave node. datapull.emr.emr_security_group_service_access(Optional) : Service access security group for EMR. This security group allows accessing spark UI through web browser. datapull.emr.emr_region(Optional) : DataPull EMR region. Needs to be specified only if datapull.application.region is not specified in master_application_config.yml or EMR region is different from datapull.application.region specified in master_application_config.yml . datapull.emr.tags(Optional) : Tag for emr cluster. Note: from the above properties properties, datapull.emr.emr_region needs to be specified only if datapull.application.region is not specified in master_application_config.yml or emr region is different from datapull.application.region specified in master_application_config.yml . In the file api/src/main/resources/terraform/datapull_task/datapull_ecs.tf set the HCL resource attribute aws_ecs_service.datapullwebapi_service.network_configuration.assign_public_ip to false if the VPC subnet id <aws_subnetid_private> is accessible from your clients' network; else set this to true Note: Setting this value to has obvious security risks since this exposes the DataPull endpoint to the internet. Use this for demo purposes only.","title":"DataPull API configuration parameters (api/src/main/resources/application-${env}.yml)"},{"location":"install_on_aws/#datapull-core-configuration-paramterscoresrcmainresourcesapplicationyml","text":"datapull.secretstore.vault(Optional) : Required only if application is using vault to access credentials. Configure the properties within this yml node to make DataPull work with vault.","title":"DataPull Core configuration paramters(core/src/main/resources/application.yml)"},{"location":"install_on_aws/#oracle-and-teradata-support","text":"For performing Data migration on Oracle and Teradata, the respective jar needs to be manually downloaded from their Company's website and add dependency in pom.xml file.","title":"Oracle and Teradata support"},{"location":"install_on_aws/#steps-to-download-ojdbc-jar","text":"Go to the url https://www.oracle.com/technetwork/apps-tech/jdbc-112010-090769.html Accept the license agreement by clicking of the radio button in front of the 'Accept License Agreement'. Oracle would ask you to create an account if you don't have one already. If you already have an account you can simply login to your account. Click on ojdbc6.jar. Download will start in few seconds.","title":"Steps to download ojdbc jar"},{"location":"install_on_aws/#steps-to-download-teradata-jar","text":"Go to the url https://downloads.teradata.com/download/connectivity/jdbc-driver. For downloading teradata jar, account needs to be created on teradata website. Downloads will be available in either .tar or .zip format. Archive file name will be in the format TeraJDBC__indep_indep_{version}.zip. Download the archive file. The archive file with contain two jar files terajdbc4.jar and tdgssconfig.jar. Both jars are needed for teradata jdbc functionality to work.","title":"Steps to download teradata jar"},{"location":"install_on_aws/#steps-to-include-oracle-into-the-project","text":"Run this command to include Oracle jar into maven repo. Run this command from folder where Oracle jars are present. Change the value of -Dversion=11.2.0.3 in the command according to downloaded jar version. Command : docker run -e MAVEN_OPTS=\"-Xmx1024M -Xss128M -XX:MetaspaceSize=512M -XX:MaxMetaspaceSize=1024M -XX:+CMSClassUnloadingEnabled\" --rm -v $(pwd):/workdir -v $HOME/.m2/:/root/.m2/ -w /workdir maven:alpine mvn install:install-file -Dfile=ojdbc6.jar -DgroupId=com.oracle -DartifactId=ojdbc6 -Dversion=11.2.0.3 -Dpackaging=jar Add below mentioned dependency to pom.xml(core/pom.xml) in DataPull core. Replace {version} with the downloaded jar version. <dependency> <groupId>com.oracle</groupId> <artifactId>ojdbc6</artifactId> <version>{version}</version> </dependency>","title":"Steps to include Oracle into the project"},{"location":"install_on_aws/#steps-to-include-teradata-into-the-project","text":"Run these commands to include Teradata jars into maven repo. Run this command from folder where Teradata jars are present. Change the value of -Dversion=16.20.00.10 in the command according to downloaded jar version. Commands : docker run -e MAVEN_OPTS=\"-Xmx1024M -Xss128M -XX:MetaspaceSize=512M -XX:MaxMetaspaceSize=1024M -XX:+CMSClassUnloadingEnabled\" --rm -v $(pwd):/workdir -v $HOME/.m2/:/root/.m2/ -w /workdir maven:alpine mvn install:install-file -Dfile=terajdbc4.jar -DgroupId=com.teradata -DartifactId=terajdbc4 -Dversion=16.20.00.10 -Dpackaging=jar docker run -e MAVEN_OPTS=\"-Xmx1024M -Xss128M -XX:MetaspaceSize=512M -XX:MaxMetaspaceSize=1024M -XX:+CMSClassUnloadingEnabled\" --rm -v $(pwd):/workdir -v $HOME/.m2/:/root/.m2/ -w /workdir maven:alpine mvn install:install-file -Dfile=tdgssconfig.jar -DgroupId=com.teradata -DartifactId=tdgssconfig -Dversion=16.20.00.10 -Dpackaging=jar Add below mentioned dependency to pom.xml(core/pom.xml) in DataPull core. Replace {version} with the downloaded jar version. <dependency> <groupId>com.teradata</groupId> <artifactId>terajdbc4</artifactId> <version>{version}</version> </dependency> <dependency> <groupId>com.teradata</groupId> <artifactId>tdgssconfig</artifactId> <version>{version}</version> </dependency>","title":"Steps to include Teradata into the project"},{"location":"install_on_aws/#create-datapull-infrastructure","text":"Have available, the AWS access key <aws_admin_access_key> and secret key <aws_admin_secret_key> of an IAM user that can create IAM users and IAM roles in your AWS account It is advisable that AWS access key <aws_admin_access_key> and secret key <aws_admin_secret_key> have admin access to the AWS account. Hence, this step would be typically done by the team managing the AWS account. Have a S3 bucket <datapull_s3_bucket> (this bucket can be an existing bucket or a new bucket) in which the folder datapull-opensource will be used to store DataPull's artifacts and logs Know a valid AWS VPC subnet id <aws_subnetid_private> in the region <aws_region> accessible to your clients, ideally not accessible from the internet; but has access to the internet Know a valid AWS security group <aws_security_grp> in the region <aws_region> accessible to your clients, ideally not accessible from the internet; but has access to the internet Have Docker installed on the machine used for deployment The Oracle ojdbc jar should be present in the resources folder of DataPull core component i.e. core/src/main/resources/ojdbc-jar/. The process of downloading ojdbc jar is explained at the end of this file.","title":"Create DataPull Infrastructure"},{"location":"install_on_aws/#create-iam-user-and-roles-with-policies","text":"From the terminal at the root of the repo, run cd api/src/main/resources/terraform/datapull_iam/ chmod +x create_user_and_roles.sh Run ./create_user_and_roles.sh <aws_admin_access_key> <aws_admin_secret_key> <aws_region> <datapull_s3_bucket> <docker_image_name> . The script will produce a ton of stdout data, that ends with Outputs: datapull_user_access_key = <datapull_user_access_key> datapull_user_secret_key = <datapull_user_secret_key> Record <datapull_user_access_key> and <datapull_user_secret_key> for use in the next section","title":"Create IAM User and Roles, with policies"},{"location":"install_on_aws/#create-aws-fargate-api-app","text":"From the terminal (assuming you are using the same terminal session as the previous steps), run cd ./../datapull_task/ If you're at the root of the repo, run cd api/src/main/resources/terraform/datapull_task/ chmod +x ecs_deploy.sh ./ecs_deploy.sh <datapull_user_access_key> <datapull_user_secret_key> <docker_image_name> <env>","title":"Create AWS Fargate API App"},{"location":"install_on_aws/#browse-to-the-datapull-api-swagger-endpoint","text":"On AWS Console, navigate to Services > Elastic Container Service > Clusters > datapullwebapi > Tasks > <id_of_task> If the HCL resource attribute aws_ecs_service.datapullwebapi_service.network_configuration.assign_public_ip was set to false, take the Private IP of the task as <datapull_api_ip> else take this as the Public IP Open the API endpoint url http://<datapull_api_ip>:8080/swagger-ui.html#!/data45pull45request45handler/startDataPullUsingPOST","title":"Browse to the DataPull API swagger endpoint"},{"location":"install_on_aws/#do-your-first-datapull","text":"Create a csv file at the S3 location s3://<datapull_s3_bucket>/datapull-opensource/data/firstdatapull/source/helloworld.csv with the following data hellofield,worldfield hello,world Post the following JSON input to the API endpoint url http://<datapull_api_ip>:8080/swagger-ui.html#!/data45pull45request45handler/startDataPullUsingPOST . Please remember to replace <your_id@DOMAIN.com> and <datapull_s3_bucket> with valid data. { \"useremailaddress\": \"<your_id@DOMAIN.com>\", \"precisecounts\": true, \"migrations\": [ { \"sources\": [ { \"platform\": \"s3\", \"s3path\": \"<datapull_s3_bucket>/datapull-opensource/data/firstdatapull/source\", \"fileformat\": \"csv\", \"savemode\": \"Overwrite\" } ], \"destination\": { \"platform\": \"s3\", \"s3path\": \"<datapull_s3_bucket>/datapull-opensource/data/firstdatapull/destination\", \"fileformat\": \"json\", \"savemode\": \"Overwrite\" } } ], \"cluster\": { \"pipelinename\": \"firstdatapull\", \"awsenv\": \"dev\", \"portfolio\": \"Data Engineering Services\", \"terminateclusterafterexecution\": \"true\", \"product\": \"Data Engineering Data Tools\", \"ComponentInfo\": \"00000000-0000-0000-0000-000000000000\" } } In approximately 8 minutes (to account for the time taken for the ephemeral EMR cluster to spin up), you should get the data from the CSV converted into JSON and written to the S3 folder s3://<datapull_s3_bucket>/datapull-opensource/data/firstdatapull/destination/ The logs for each DataPull invocation are available at s3://<datapull_s3_bucket>/datapull-opensource/logs/DataPullHistory . The logs for each migration with DataPull invocations are available at s3://<datapull_s3_bucket>/datapull-opensource/logs/MigrationHistory If you had configured the anonymous SMTP server for DataPull to send you email reports, you could get a report with the subject DataPull Report - firstdatapull (application_<random_numbers>)","title":"Do your first DataPull"},{"location":"transformation/","text":"{ \"useremailaddress\": \"your_email_address\", \"migrations\": [ { \"source\": { \"platform\": \"elastic\", \"clustername\":\"cluster_name\", \"port\": \"9201\", \"login\": \"user_id\", \"password\": \"password\", \"index\": \"test\", \"type\": \"docs\", \"version\": \"6.3.0\", \"alias\":\"U\" }, \"sql\": { \"query\": \"SELECT id, name, DATE_FORMAT(current_date(), \\\"y-MM-dd HH:mm:ss.SSS\\\") as date FROM U\" }, \"destination\": { \"platform\": \"mssql\", \"awsenv\": \"test\", \"server\": \"server_name\", \"database\": \"database\", \"table\": \"destination_table\", \"login\": \"user_id\", \"password\": \"password\" } } ], \"cluster\": { \"pipelinename\": \"emr_cluster_name\", \"awsenv\": \"dev\", \"portfolio\": \"portfolio_name\", \"product\": \"product_name\", \"ec2instanceprofile\": \"ec2_instance_profile\", \"terminateclusterafterexecution\": \"false\", \"ComponentInfo\":\"YOUR_Component-UUID_dominion\" } }","title":"Sample input JSON"}]}